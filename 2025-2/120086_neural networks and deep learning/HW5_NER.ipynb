{"cells":[{"cell_type":"markdown","metadata":{"id":"0NvJ_mnDvy4P"},"source":["# HW5: Named-Entity Recognition\n","\n","이번 과제에서는 GRU(gated recurrent unit)를 이용해 NER(named-entity recognition) 작업을 해보도록 하겠다. NER은 문장에서 명명된 개체를 찾아서 사람 이름, 조직, 위치, 의료 코드, 시간 표현, 수량, 화폐 가치, 백분율 등과 같이 미리 정의된 카테고리로 분류하는 정보 추출 작업이다.\n","\n","<br/>\n","<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*oyOE7LB7-KKRETBpsHRJPg.png\" />\n","<br/>\n","\n","**이번 과제에서는 GPU를 이용한다. 이를 위해서는 메뉴의 [런타임]-[런타임 유형 변경]에서 '하드웨어 가속기'를 'T4 GPU'로 선택해야 한다.**\n","\n","우선 필요한 모듈을 불러오고, 필요한 셋팅을 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONkJs3T49P4X"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt # Matplotlib\n","import pandas as pd\n","import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"cMrbltOi9Tho"},"source":["## 데이터셋 로딩\n","\n","이번 과제에서는 [Kaggle](https://www.kaggle.com/)에서 제공하는 [NER 데이터셋](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus)을 사용한다.\n","\n","우선 데이터셋을 다운로드 받고 압축을 푼다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZZAu1JBYgyM"},"outputs":[],"source":["!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1hBpe9j9wbOw4HhCKcbP_D0mog0_-IR9Q' -O ner_dataset.csv"]},{"cell_type":"markdown","metadata":{"id":"6_2urvVaoBgY"},"source":["데이터셋은 **ner_dataset.csv** 파일로 저장되며, CSV(comma-seperated values) 형식을 갖는다. 이는 엑셀을 통해서도 열어볼 수 있다. 이번 과제에서는 CSV 파일을 쉽게 처리할 수 있도록, 데이터 분석에 널리 사용하는 [Pandas](https://pandas.pydata.org/) 라이브러리를 사용한다. Pandas는 2차원 테이블 형태의 데이터를 [Data Frame 구조](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)로 관리한다.\n","\n","우선 **ner_dataset.csv** 파일을 불러온다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0UWFEAga6fI"},"outputs":[],"source":["data = pd.read_csv('ner_dataset.csv', encoding='unicode_escape') # CSV 파일을 불러온다.\n","data.head(10) # 일부 데이터를 출력한다."]},{"cell_type":"markdown","metadata":{"id":"2z4-SR6mLPJh"},"source":["데이터는 4개의 열로 구성되어 있다. **Sentence #** 열은 문장 번호를 나타낸다. 이 때, **Sentence: #** 값은 다음과 같은 패턴을 가지고 있다. **Sentence: 1**이 나타나고 나서 **NaN** 값이 계속 나오다가, 다시 **Sentence: 2**가 나타난다. 그리고 다시 **NaN** 값이 나온다. 이는 하나의 문장을 [토큰(token)](https://ko.wikipedia.org/wiki/%EB%82%B1%EB%A7%90_%EB%B6%84%EC%84%9D)으로 나눠놓은 것으로, 'Sentence: 숫자'부터 다음 'Sentence: 숫자'가 나오기 전까지가 하나의 문장을 구성한다.\n","\n","**Word** 열은 문장을 구성하는 토큰(token)을 나타낸다. **POS** 열은 본 과제에서는 필요가 없다. **Tag** 열은 토큰을 분류한 정보로 그 의미는 다음과 같다:\n","\n","|Tag|의미|\n","|------|---|\n","|O|분류 안됨|\n","|geo|Geographical Entity|\n","|org|Organization|\n","|per|Person|\n","|gpe|Geopolitical Entity|\n","|tim|Time indicator|\n","|art|Artifact|\n","|eve|Event|\n","|nat|Natural Phenomenon|\n","\n","**Tag** 앞에 붙은 **I**와 **B**는 무시한다.\n","\n","**Sentence: 1**의 문장을 보면 다음과 같이 해석할 수 있다:\n","\n","|Thousands|of|demonstrators|have|marched|through|London|...|\n","|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n","|O|O|O|O|O|O|geo|...|"]},{"cell_type":"markdown","metadata":{"id":"eB1RaD--Sd7q"},"source":["## 단어 인덱싱(Word Indexing)\n","\n","Neural network은 문자로 표현된 단어를 직접 입력 데이터로 사용할 수 없기 때문에, 단어를 수치로 변환해야 한다. 따라서, NER 데이터셋의 **Word** 열과 **Tag** 열의 단어들을 수치로 변환한다.\n","\n","우선, 데이터셋에 있는 단어들을 중복되지 않게 추출한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DRVq90NSmgg"},"outputs":[],"source":["vocab_tokens = list(set(data['Word'].to_list())) # 중복되는 토큰(Word 열)을 제거한다.\n","vocab_tags = list(set(data['Tag'].to_list())) # 중복되는 태그(Tag 열)를 제거한다.\n","\n","print(\"중복되지 않는 토큰 갯수:\", len(vocab_tokens)) # 35178\n","print(\"중복되지 않는 태그 갯수:\", len(vocab_tags)) # 17"]},{"cell_type":"markdown","metadata":{"id":"0qMVdAFnTC86"},"source":["단어들에 순서대로 번호를 매겨 단어 사전을 만든다. 이 때, 나중에 사용할 **PAD**라는 특수한 토큰을 먼저 추가한다.\n","\n","작업 후, *token2index*에는 각 토큰에 대한 인덱스 변환 정보가 저장되고, *tag2index*에는 태그에 대한 인덱스 변환 정보가 저장된다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zpw9Q4t5TT68"},"outputs":[],"source":["def create_index(vocabs):\n","  vocab2index = {}\n","  vocab2index['<PAD>'] = 0\n","  for index, vocab in enumerate(vocabs):\n","    vocab2index[vocab] = index+1\n","\n","  return vocab2index\n","\n","token2index = create_index(vocab_tokens) # 토큰 to 인덱스 사전\n","tag2index = create_index(vocab_tags) # 태그 to 인덱스 사전"]},{"cell_type":"markdown","metadata":{"id":"ddngJPu-AZz3"},"source":["사전의 단어 갯수를 확인한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZ-j7HaHAZM9"},"outputs":[],"source":["print(\"토큰 갯수:\", len(token2index)) # 35179\n","print(\"태그 갯수:\", len(tag2index)) # 18"]},{"cell_type":"markdown","metadata":{"id":"tatapzNTVcFl"},"source":["다음 코드는 *token2index*와 *tag2index*의 일부 내용을 출력한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMgoUiHGUktS"},"outputs":[],"source":["print(\"Word to index\")\n","print([pair for pair in list(token2index.items())[:5]])\n","\n","print(\"Tag to index\")\n","print([pair for pair in list(tag2index.items())[:5]])"]},{"cell_type":"markdown","metadata":{"id":"aLtbR1CdVhRw"},"source":["*data*에 *token2index*와 *tag2index*를 새 열로 추가한다. 학습에는 **Word** 열과 **Tag** 열 대신 **Word_index** 열과 **Tag_index** 열을 사용한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GKY7BxeSEp3"},"outputs":[],"source":["data['Word_index'] = data['Word'].map(token2index)\n","data['Tag_index'] = data['Tag'].map(tag2index)\n","data.head(10)"]},{"cell_type":"markdown","metadata":{"id":"GmjtCf8sWQQC"},"source":["## 문장 추출\n","\n","같은 문장에 있는 토큰을 추출하여 문장을 만들기 위해 데이터셋을 가공한다. 우선 **Sentence #** 열에 **NaN**으로 된 값을 채워준다. 이는 Pandas의 [ffill](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html)을 이용한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuUbnM1KWUsL"},"outputs":[],"source":["data_fill = data.ffill(axis=0)\n","\n","data_fill.head() # 일부 데이터 출력"]},{"cell_type":"markdown","metadata":{"id":"w81PuQvubiDf"},"source":["같은 **Sentence #** 값을 갖는 행을 그룹핑 해준다. 이는 Pandas의 [groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)로 할 수 있다. 그리고 같은 그룹에 속하는 열 값들을 Pandas의 [agg](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html)를 이용해 리스트로 묶어준다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxvHwQJzbJya"},"outputs":[],"source":["data_group = data_fill.groupby(['Sentence #']) # Sentence # 열로 그룹핑\n","# Word, POS, Tag, Word_index, Tag_index 열의 값들을 list로 묶음\n","data_group = data_group[['Word', 'POS', 'Tag', 'Word_index', 'Tag_index']].agg(lambda x: list(x))\n","\n","data_group.head() # 일부 데이터 출력"]},{"cell_type":"markdown","metadata":{"id":"uLZkmZ5NoxU6"},"source":["## 데이터셋 가공\n","\n","GRU를 이용해 미니배치 단위로 학습하기 위해서는 문장의 길이가 모두 동일해야 한다. 따라서, 모든 문장을 동일한 길이를 갖도록 가공해야 한다. 여기서는 가장 긴 문장의 길이를 기준으로, 다른 모든 문장의 길이를 맞춰준다. 그리고 짧은 문장의 경우, 비게 되는 나머지 공간들을 앞에서 정의한 **PAD** 토큰으로 채운다.\n","\n","우선 데이터 처리를 쉽게 하기 위해 *data_group*을 리스트로 변환한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScSlZc40qpm0"},"outputs":[],"source":["# data_group의 Word_index는 한 문장을 구성하는 토큰들의 인덱스 리스트가 저장되어 있다.\n","# 따라서 sentences는 토큰들의 인덱스 리스트의 리스트이다.\n","sentences = data_group['Word_index'].tolist()\n","\n","# data_groupd의 태그도 리스트로 변환한다.\n","tags = data_group['Tag_index'].tolist()\n","\n","print(sentences[:1]) # 첫 번째 문장을 출력 (인덱스로 표현됨)\n","print(tags[:1]) # 첫 번째 문장에 대한 태그들을 출력 (인덱스로 표현됨)"]},{"cell_type":"markdown","metadata":{"id":"Sf5jbKGxWYlU"},"source":["가장 긴 문장의 길이를 계산한다.\n","\n","**지시: 가장 긴 문장의 길이를 계산할 수 있게 아래 코드를 완성한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SgXFGj-uWcLI"},"outputs":[],"source":["## sentences에 저장된 문장들 중, 가장 길이가 긴 문장의 길이를 계산하여 max_len에 저장한다.\n","## 이를 위해, 아래 코드를 적절히 수정하시오.\n","#### 코드 시작 ####\n","max_len = 1\n","#### 코드 종료 ####\n","\n","assert max_len == 104\n","print(\"최대 길이:\", max_len)"]},{"cell_type":"markdown","metadata":{"id":"V3FhuKyhWtqY"},"source":["다른 문장들도 길이를 최대 길이 *max_len*으로 맞춰준다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Mc2q6A2XZ_I"},"outputs":[],"source":["# padding 작업을 해주는 함수\n","def pad_sequence(sequences, max_len, pad_value):\n","  for s in sequences:\n","    if len(s) < max_len: # 최대 길이보다 짧으면\n","      s += [pad_value] * (max_len - len(s)) # 나머지는 전부 'PAD' 토큰으로 채운다.\n","\n","  return sequences\n","\n","# Padding 전 첫 문장의 길이\n","print(\"Padding 전:\", len(sentences[0]))\n","\n","sentences = pad_sequence(sentences, max_len, token2index['<PAD>'])\n","tags = pad_sequence(tags, max_len, tag2index['<PAD>'])\n","\n","# Padding 후 첫 문장의 길이\n","print(\"Padding 전:\", len(sentences[0]))\n","print(sentences[:1])\n","print(tags[:1])"]},{"cell_type":"markdown","metadata":{"id":"kOS6__8BzRFD"},"source":["위 결과를 보면 문장의 뒷부분이 **0**으로 채워져 있는 것을 볼 수 있다.\n","\n","데이터셋을 학습용셋(trainset), 검증용셋(validationset), 테스트셋(testset)으로 분할한다. 이를 쉽게 하기 위해, [torch.utils.data.dataset.random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split)을 이용한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mJgLbw_V0L97"},"outputs":[],"source":["sentences = torch.tensor(sentences) # 텐서로 변환한다.\n","tags = torch.tensor(tags) # 텐서로 변환한다.\n","\n","ds = torch.utils.data.StackDataset(sentences, tags) # sentences와 tags를 묶어준다.\n","\n","trainset, validationset, testset = torch.utils.data.dataset.random_split(ds, [0.6, 0.2, 0.2]) # 데이터셋을 60:20:20으로 분할한다.\n","\n","print(\"Traning set 개수:\", len(trainset))\n","print(\"Validation set 개수:\", len(validationset))\n","print(\"Test set 개수:\", len(testset))"]},{"cell_type":"markdown","metadata":{"id":"6-LLOCTp1mMd"},"source":["## Hyperparameters\n","학습에 필요한 추가 hyperparameter를 설정한다.\n","\n","**지시: 좋은 성능을 낼 수 있게 아래 hyperparameter를 조정한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djwKxYcJ1r6C"},"outputs":[],"source":["# Hyperparameter를 설정한다.\n","# 사용자가 변경할 수 있다.\n","epochs = 2 # 최대 epoch\n","learning_rate = 0.1 # Learning rate\n","batch_size = 32 # Batch size\n","embedding_size = 32\n","hidden_size = 32 # hidden state size"]},{"cell_type":"markdown","metadata":{"id":"o9IkL_b41TBr"},"source":["## 모델 정의\n","\n","NER을 수행하기 위한 RNN 모델을 정의한다. 여기서는 수업 시간에 배운 many-to-many 구조를 정의한다. 이 때, 모델의 입력은 문장의 토큰들이고, 모델의 출력은 각 토큰들에 대한 태그가 된다. RNN 모델을 정의하기 위해 [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)에서 상속받은 클래스를 정의한다. 그리고 *\\_\\_init\\_\\_()* 함수 안에 네트워크의 레이어들을 정의하고, *forward()* 함수 안에서 정의한 레이어들을 연결해 준다.\n","\n","NER을 위해 GRU([nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html))를 이용한다. 그러나 GRU에는 one-hot encoding 형태의 word embedding 결과가 들어가야 한다. 따라서 인덱스로 표현된 입력 토큰을 word embedding 하는 과정이 필요하다. 이는 PyTorch의 [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)을 이용한다. 그리고 GRU의 출력을 이용해 fully-connected 레이어를 구성한다. 즉, 대략적인 구성은 다음과 같다.\n","<br/><br/>\n","X -> nn.Embedding -> nn.GRU -> nn.Linear -> nn.ReLU -> nn.Linear -> y\n","<br/><br/>\n","*주의: 마지막에 softmax를 추가하지 않는다. 이는 [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)가 대신 해준다.*\n","\n","위의 구조는 하나의 예시일 뿐, 얼마든지 변형할 수 있다. 예를 들면, GRU 레이어의 수를 늘릴 수 있고(deep RNN), 양방향(bidirectional RNN)이 되게 할 수도 있다. 또한, Dropout 및 Batch Normalization을 사용할 수도 있다.\n","\n","\n","**지시: 좋은 성능을 낼 수 있게 다음 모델을 정의한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eq5tAHjG1V6S"},"outputs":[],"source":["class NerModel(nn.Module): # nn.Module을 상속받은 클래스를 정의. 이름을 Ner로 정의하였다.\n","  def __init__(self, input_size, embedding_size, hidden_size, output_size): # 이 안에 레이어들을 정의한다.\n","    super(NerModel, self).__init__()\n","\n","    self.input_size = input_size\n","    self.embedding_size = embedding_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","\n","    ## 아래 코드를 적절히 수정하시오.\n","    #### 코드 시작 ####\n","    self.embedding = nn.Embedding(input_size, embedding_size)\n","    self.gru = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, batch_first=True) # GRU\n","\n","    # GRU에서 나온 출력값을 아래의 fully-connected layer에 연결한다.\n","    self.linear = nn.Sequential(\n","        nn.Linear(self.hidden_size, self.hidden_size),\n","        nn.ReLU(),\n","        nn.Linear(self.hidden_size, self.output_size)\n","        )\n","    #### 코드 종료 ####\n","\n","  # Hidden state를 초기화 시켜주는 함수\n","  def init_hidden(self, batch_size):\n","    return torch.zeros(1, batch_size, self.hidden_size) # hidden은 batch_first가 True이어도 batch_size가 두 번째 온다.\n","\n","  def forward(self, x): # 이 안에서 레이어들을 연결해준다.\n","    h0 = self.init_hidden(x.shape[0]).to(x.device) # 처음 hidden state를 설정한다. 이 값은 모두 0으로 되어 있다.\n","\n","    ## 아래 코드를 적절히 수정하시오.\n","    #### 코드 시작 ####\n","\n","    embedded = self.embedding(x)\n","    gru_out, hn = self.gru(embedded, h0)\n","    y_pred = self.linear(gru_out)\n","\n","    #### 코드 종료 ####\n","    return y_pred\n","\n","model = NerModel(input_size=len(token2index), embedding_size=embedding_size, hidden_size=hidden_size, output_size=len(tag2index)) # 모델 생성"]},{"cell_type":"markdown","metadata":{"id":"peSiFniRCJGN"},"source":["모델을 테스트한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsjDZCCrCJz1"},"outputs":[],"source":["x = torch.randint(low=0, high=len(token2index), size=(batch_size, max_len))\n","output = model(x)\n","\n","assert output.shape == (32, 104, 18)\n","\n","print(output.shape) # (32, 104, 18)"]},{"cell_type":"markdown","metadata":{"id":"4vBu0HzVE7kB"},"source":["## 학습\n","\n","데이터를 batch size 단위로 나누어 불러올 수 있게 [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)를 사용한다. Training set과 validation set 모두에 대해 각각의 DataLoader를 만들어준다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dytIXkpxFCwt"},"outputs":[],"source":["# shuffle=True는 데이터를 불러올 때, 데이터를 섞어준다.\n","trainset_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n","validationset_loader = torch.utils.data.DataLoader(dataset=validationset, batch_size=batch_size, shuffle=True)\n","\n","for X, y in trainset_loader: # dataloader를 이용해 batch size 만큼 입력 데이터 X와 출력 데이터 y를 가져온다.\n","  print(X.shape) # (32, 104)\n","  print(y.shape) # (32, 104)\n","  break"]},{"cell_type":"markdown","metadata":{"id":"67B5of4hGE7q"},"source":["Optimizer를 정의한다.\n","\n","**지시: 좋은 성능을 낼 수 있게 최적화 알고리즘을 선택한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78Ozhq6PGGr7"},"outputs":[],"source":["## 아래 코드를 적절히 수정하시오.\n","#### 코드 시작 ####\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","#### 코드 종료 ####"]},{"cell_type":"markdown","metadata":{"id":"JlGDdo8WGLMg"},"source":["Loss function을 정의한다. 여기서는 PyTorch가 제공하는 [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)를 사용한다. 이는 자체적으로 softmax 함수를 포함하므로, 모델에 softmax 활성화 함수를 별도로 추가할 필요가 없다. 또한, y를 one-hot encoding으로 변환하지 않아도 내부적으로 변환해 처리해 준다.\n","\n","그러나 loss를 계산할 때, 빈 값으로 채운 **PAD** 부분을 제외시켜 주서야 한다. 이는 *ignore_index*을 사용하면 loss 계산 시 **PAD** 부분을 제외시켜준다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAl0HTodGSBJ"},"outputs":[],"source":["loss_function = nn.CrossEntropyLoss(ignore_index=token2index['<PAD>'])"]},{"cell_type":"markdown","metadata":{"id":"laql6zXL4MUy"},"source":["Loss function을 테스트한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s01TzEVE4MUy"},"outputs":[],"source":["output_test = torch.rand(size=(batch_size, max_len, len(tag2index))).view(-1, len(tag2index))\n","y_test = torch.randint(0, len(tag2index), (batch_size, max_len)).view(-1)\n","\n","print(\"output shape:\", output_test.shape) # (3328, 18)\n","print(\"y shape:\", y_test.shape) # (3328)\n","\n","loss_test = loss_function(output_test, y_test)\n","print(\"loss shape:\", loss_test)"]},{"cell_type":"markdown","metadata":{"id":"LFHLiTApLiuZ"},"source":["한 번의 epoch에 대해 데이터를 batch size 단위로 모델을 학습시키는 함수를 만든다. 이 함수는 dataloader에서 batch size 만큼 데이터를 가져와 optimizer를 이용해 학습한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLgel4QOLnL0"},"outputs":[],"source":["def train_loop(dataloader, model, loss_func, optimizer):\n","  total_mask = 0 # PAD를 제외한 토큰의 갯수 저장\n","  training_loss = 0 # Loss를 저장하기 위한 변수\n","  correct = 0 # 정답을 맞춘한 개수를 저장\n","\n","  model.train() # 학습한다는 것을 표시함\n","\n","  for X, y in dataloader: # dataloader를 이용해 batch size 만큼 입력 데이터 X와 출력 데이터 y를 가져온다.\n","    X = X.cuda() # X를 GPU로 이동\n","    y = y.cuda() # y를 GPU로 이동\n","\n","    optimizer.zero_grad() # Gradient를 0으로 초기화\n","    output = model(X) # 모델을 이용해 forward propagation 수행\n","\n","    output_view = output.view(-1, len(tag2index)) # loss 계산을 쉽게 하기 위해 output의 모양을 변환\n","    y_view = y.view(-1) # loss 계산을 쉽게 하기 위해 y의 모양을 변환\n","\n","    loss = loss_func(output_view, y_view) # loss 계산\n","    loss.backward() # Backpropagation 수행\n","    optimizer.step() # 가중치 업데이트\n","\n","    mask = (y_view != token2index['<PAD>']) # PAD가 아닌 위치를 구한다.\n","    count_mask = mask.sum().item() # PAD가 아닌 부분의 갯수를 계산한다.\n","    total_mask += count_mask # PAD가 아닌 부분의 갯수를 누적시킨다.\n","\n","    training_loss += loss.item() * count_mask # Loss를 누적시킨다.\n","\n","    prediction = output_view.argmax(1) # 예측한 결과\n","\n","    # PAD가 아닌 부분에 대해서만, 예측한 결과와 출력 데이터(정답)을 비교해 맞은 개수를 찾는다.\n","    correct += (prediction == y_view).masked_select(mask).sum().item()\n","\n","  training_loss /= total_mask # 평균 loss를 계산\n","  accuracy = correct / total_mask # 정확도를 계산\n","\n","  return training_loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"zBfLaZi04MUz"},"source":["한 번의 epoch에 대해 데이터를 batch size 단위로 모델을 평가하는 함수를 만든다. 이 함수는 dataloader에서 batch size 만큼 데이터를 가져와 학습된 모델을 이용해 loss 및 정확도를 계산한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJhhcXqd4MU0"},"outputs":[],"source":["def test_loop(dataloader, model, loss_func):\n","  total_mask = 0\n","  test_loss = 0 # Loss를 저장하기 위한 변수\n","  correct = 0 # 정답을 맞춘한 개수를 저장\n","\n","  model.eval() # 평가한다는 것을 표시함. 이는 dropout과 같이 학습과 평가 시 적용이 다르게 되는 레이에어 영향을 준다.\n","\n","  with torch.no_grad(): # 미분을 수행하지 않음\n","    for X, y in dataloader: # dataloader를 이용해 batch size 만큼 입력 데이터 X와 출력 데이터 y를 가져온다.\n","      X = X.cuda() # X를 GPU로 이동\n","      y = y.cuda() # y를 GPU로 이동\n","\n","      output = model(X) # 모델을 이용해 forward propagation 수행하여 결과 예측\n","\n","      output_view = output.view(-1, len(tag2index)) # loss 계산을 쉽게 하기 위해 output의 모양을 변환\n","      y_view = y.view(-1) # loss 계산을 쉽게 하기 위해 y의 모양을 변환\n","\n","      mask = (y_view != token2index['<PAD>']) # PAD가 아닌 위치를 구한다.\n","      count_mask = mask.sum().item() # PAD가 아닌 부분의 갯수를 계산한다.\n","      total_mask += count_mask # PAD가 아닌 부분의 갯수를 누적시킨다.\n","\n","      loss = loss_func(output_view, y_view) # loss 계산\n","      test_loss += loss.item() * count_mask # Loss를 누적시킨다.\n","\n","      prediction = output_view.argmax(1) # 예측한 결과\n","      # PAD가 아닌 부분에 대해서만, 예측한 결과와 출력 데이터(정답)을 비교해 맞은 개수를 찾는다.\n","      correct += (prediction == y_view).masked_select(mask).sum().item()\n","\n","    test_loss /= total_mask # 평균 loss를 계산\n","    accuracy = correct / total_mask # 정확도를 계산\n","\n","  return test_loss, accuracy"]},{"cell_type":"markdown","metadata":{"id":"tW4tUXJCOOY6"},"source":["Training set을 이용해 모델을 학습하고, validation set을 이용해 검증하는 루프를 구현한다. 그리고, 매 epoch 마다 loss 및 accuracy를 저장한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGBT0yjSORGk"},"outputs":[],"source":["# 학습 상황을 저장하기 위한 변수\n","train_loss_history = []\n","train_accuracy_history = []\n","validation_loss_history = []\n","validation_accuracy_history = []\n","\n","model = model.cuda()\n","\n","for epoch in range(1, epochs+1): # 최대 epoch만큼 반복한다.\n","  print(\"Epoch: \", epoch)\n","\n","  train_loss, train_accuracy = train_loop(trainset_loader, model, loss_function, optimizer) # train set을 이용해 학습한다.\n","  train_loss_history.append(train_loss) # train set에 대한 loss를 저장한다.\n","  train_accuracy_history.append(train_accuracy) # train set에 대한 accuracy를 저장한다.\n","\n","  validation_loss, validation_accuracy = test_loop(validationset_loader, model, loss_function) # validation set을 이용해 검증한다.\n","  validation_loss_history.append(validation_loss) # validation set에 대한 loss를 저장한다.\n","  validation_accuracy_history.append(validation_accuracy) # validation set에 대한 accuracy를 저장한다.\n","\n","  print(f\"Training accuracy: {train_accuracy}, Training loss: {train_loss}\")\n","  print(f\"Validation accuracy: {validation_accuracy}, Validation loss: {validation_loss}\")"]},{"cell_type":"markdown","metadata":{"id":"08aSsnZW4MU1"},"source":["매 epoch 마다 loss와 accuracy가 변하는 과정을 그래프로 보여준다. Epoch이 진행됨에 따라서 loss는 감소하고 accuracy는 증가한다. 그리고 validation loss는 training loss보다 일반적으로 크다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXt_j7C04MU1"},"outputs":[],"source":["x = np.arange(1, len(train_loss_history)+1, dtype=int)\n","\n","# Loss 그래프\n","_, ax = plt.subplots()\n","ax.plot(x, train_loss_history, label=\"training\")\n","ax.plot(x, validation_loss_history, label=\"validation\")\n","ax.set_title('Loss')\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Loss')\n","ax.legend()\n","plt.show()\n","\n","# Accuracy 그래프\n","_, ax = plt.subplots()\n","ax.plot(x, train_accuracy_history, label=\"training\")\n","ax.plot(x, validation_accuracy_history, label=\"validation\")\n","ax.set_title('Accuracy')\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Accuracy')\n","ax.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"C9Y9yNou4MU2"},"source":["## 평가\n","\n","평가에는 test set을 이용한다. 이를 batch size 단위로 불러오기 위해 dataloader를 만든다. 그러나 학습이 아니기 때문에 batch size 단위로 평가할 필요는 없다. 단지, 한 번에 메모리로 불러오는 부담을 줄이기 위해서이다.\n","\n","**지시: Test accuracy가 95%가 넘도록 모델 및 하이퍼파라미터를 설정한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQEn8cbS4MU2"},"outputs":[],"source":["model.cuda()\n","\n","testset_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False)\n","test_loss, test_accuracy = test_loop(testset_loader, model, loss_function) # test set을 이용해 평가한다.\n","\n","print(f\"Test accuracy: {test_accuracy}, Test loss: {test_loss}\")"]},{"cell_type":"markdown","metadata":{"id":"9EhHZ9pU4MU2"},"source":["Test set에 있는 일부 문장 중, 임의로 한 개 선택해서 예측이 잘 되는지 확인해 본다.\n","\n","우선 테스트셋에서 한 개의 문장을 가져온다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4eIqyqJ4MU2"},"outputs":[],"source":["import random\n","\n","r = random.randint(0, len(testset)) # 임의의 문장 번호를 선택\n","X_test, y_test = testset[r] # 임의의 데이터를 가져온다.\n","\n","# index에서 token을 찾는 사전 생성\n","# index2token은 학습에는 필요 없으나, 학습 및 추론 결과를 사용자에게 보여주기 위해 필요하다.\n","# 즉, 인덱스로 표현된 문장을 사용자가 볼 수 있는 문자로 변환할 때 사용한다.\n","index2token = {v:k for k,v in token2index.items()}\n","\n","# X_test를 문장으로 변환한다.\n","tokens = []\n","for index in X_test.tolist():\n","    if index == 0: break # <PAD>이면 종료\n","    tokens.append(index2token[index])\n","\n","# 토큰을 연결하여 문장을 만든다.\n","sentence = ' '.join(tokens)\n","print(sentence)"]},{"cell_type":"code","source":["X_test.shape"],"metadata":{"id":"iFJlUn1sBIk7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKJOMSgb4MU2"},"source":["테스트 데이터에 대해 NER 작업을 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXm391b64MU2"},"outputs":[],"source":["model.cpu()\n","model.eval()\n","\n","with torch.no_grad():\n","  # X_test의 shape에는 batch size에 대한 차원이 없으므로, 이를 추가해줘야 한다.\n","  # 즉, (104)를 (1, 104)로 변환한다.\n","  X_sample = X_test.unsqueeze(dim=0)\n","  output = model(X_sample)\n","\n","output_view = output.view(-1, len(tag2index))\n","prediction = output_view.argmax(1) # 예측한 결과\n","prediction = prediction[:len(tokens)].tolist()\n","\n","index2tag = {v:k for k,v in tag2index.items()} # index에서 tag를 찾는 사전 생성\n","\n","predicted_tags = [index2tag[index] for index in prediction]\n","print(\"예측:\", predicted_tags)\n","\n","y_sample = y_test[:len(tokens)].tolist()\n","true_tags = [index2tag[index] for index in y_sample]\n","print(\"정답:\", true_tags)"]},{"cell_type":"markdown","metadata":{"id":"Nna4I4Ex4MU3"},"source":["문장에 태그를 붙여 출력한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7YESCjN4MU3"},"outputs":[],"source":["from IPython.display import Markdown\n","\n","dressed_sentence = \"\"\n","\n","for token, tag in zip(tokens, predicted_tags):\n","  if tag != 'O':\n","    token = token + \"[\" + tag + \"]\"\n","\n","  dressed_sentence = dressed_sentence + token + ' '\n","\n","display(Markdown(dressed_sentence))"]},{"cell_type":"markdown","metadata":{"id":"w8wXW3adIo3I"},"source":["이번 과제는 여기까지 입니다. 수고하셨습니다."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}