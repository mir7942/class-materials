{"cells":[{"cell_type":"markdown","metadata":{"id":"ByB40Kmg-n2V"},"source":["수정일: 2025.09.02"]},{"cell_type":"markdown","metadata":{"id":"G5YrsnIy3M6D"},"source":["#HW1: Housing Price Prediction"]},{"cell_type":"markdown","metadata":{"id":"yeE3Vei53Sur"},"source":["이번 과제에서는 linear regression을 이용해 집값을 예측하려고 한다. 아래 나오는 내용들을 잘 읽어보고 지시에 따라 코드를 완성해 보자.\n","\n","우선 필요한 모듈을 불러오고, 필요한 셋팅을 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgZ7V84h3Pc9"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","import urllib # 데이터셋을 다운로드 받기 위해 필요"]},{"cell_type":"markdown","metadata":{"id":"JKhTP7p84Lgq"},"source":["## 데이터\n","아래 코드를 실행시키면, 과제에 필요한 데이터를 다운로드한다. 데이터 파일의 이름은 housing_prices.txt이다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZ1U4bRW4Xxm"},"outputs":[],"source":["!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1IhCTHicT0lVt_SAYwaLNPokyC-TOMJGL' -O housing_prices.txt"]},{"cell_type":"markdown","metadata":{"id":"ou-iYHpZ4zp9"},"source":["다운로드 받은 파일을 읽고, 잘 읽었는지 확인하기 위해 일부 데이터를 출력한다.\n","\n","housing_prices.txt 파일 안에는 데이터가 텍스트 파일로 저장되어 있으며, 각각의 값은 쉼표(comma)로 구분된다. 이러한 파일은 NumPy의 [loadtxt](https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html) 함수로 불러올 수 있다.\n","\n","데이터의 첫 번째 열(column)은 집의 크기(ft<sup>2</sup>), 두 번째 열은 방의 개수, 세 번째 열은 집의 가격($)을 나타낸다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qlECJWM4edi"},"outputs":[],"source":["dataset = np.loadtxt(\"housing_prices.txt\", dtype=np.float64, delimiter=\",\")\n","dataset[:5, ::]"]},{"cell_type":"markdown","metadata":{"id":"GIBJveF06R6j"},"source":["불러온 데이터를 입력 변수(input variable)와 출력 변수(output variable)로 분리한다. 이는 NumPy의 [Slicing](https://numpy.org/doc/stable/user/basics.indexing.html#slicing-and-striding)을 이용하면 된다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTqtTl7T6wBp"},"outputs":[],"source":["X = dataset[:, :2] # 첫 번째와 두 번째 열. 크기가 (데이터 개수, 2)\n","Y = dataset[:, -1] # 마지막 열. 크기가 (데이터 개수, 1)\n","m, n = X.shape # 데이터 개수와 feature의 수"]},{"cell_type":"markdown","metadata":{"id":"x2VZdN4B7vnp"},"source":["분리한 데이터의 일부를 출력해본다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7qS6A8s7uxL"},"outputs":[],"source":["print(\"X의 크기:\", X.shape)\n","print(\"Y의 크기:\", Y.shape)\n","print(\"데이터 개수:\", m)\n","print(\"Feature 수:\", n)\n","\n","print(\"X:\")\n","print(X[:5,:])\n","\n","print(\"Y:\")\n","print(Y[:5])"]},{"cell_type":"markdown","metadata":{"id":"PIEe03-49Vj9"},"source":["불러온 데이터를 그래프로 출력한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFOit2au78Lz"},"outputs":[],"source":["fig, axes = plt.subplots()\n","\n","axes.scatter(X[:, 0], Y)\n","axes.set_xlabel(\"Size(X1)\")\n","axes.set_ylabel(\"Price(Y)\")\n","plt.show()\n","\n","fig, axes = plt.subplots()\n","\n","axes.scatter(X[:, -1:], Y)\n","axes.set_xlabel(\"Number of Bedrooms(X2)\")\n","axes.set_ylabel(\"Price(Y)\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"owuzTNvv9zI6"},"source":["## Feature Scaling\n","데이터를 보면 x<sub>1</sub>과 x<sub>2</sub>의 데이터 범위가 크게 차이가 난다. 따라서 feature scaling을 통해 값의 범위를 조정해 줄 것이다. 여기서는 mean normalization을 이용해 feature scaling을 한다. Mean normalization에서는 각 feature 또는 열에 대해 평균 $\\mu$과 표준편차 $\\sigma$를 계산하고, 다음 식을 이용해 값을 변경한다.\n","\n","$x_i=\\cfrac{x_i-\\mu_i}{\\sigma_i}$\n","\n","평균은 NumPy의 [mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) 함수를 이용하면 되고, 표준편차는 [std](https://numpy.org/doc/stable/reference/generated/numpy.std.html) 함수를 이용하면 된다. 단, axis 값을 적절하게 설정해야 한다.\n","\n","x<sub>1</sub>과 x<sub>2</sub>의 값이 변경되었으므로, 원래 값과의 관계를 알기 위해 평균과 표준편차를 저장해 놓아야 한다.\n","\n","**지시: feature_scaling을 해주는 아래 함수를 완성한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPbFdA9u-sQN"},"outputs":[],"source":["def feature_scaling(X):\n","  '''\n","  입력 변수 X를 feature scaling한 후 값을 반환하는 함수\n","\n","  매개변수:\n","    X: 입력 변수, NumPy 2차원 배열\n","\n","  반환:\n","    feature scaling한 X, NumPy 2차원 배열\n","    X의 각 열에 대한 평균값, 스칼라 1차원 배열\n","    X의 각 열에 대한 표준편차, 스칼라 1차원 배열\n","  '''\n","\n","  ## X의 각 열에 대한 평균을 계산하여 변수 mean에 저장할 수 있도록,\n","  ## 아래 코드를 적절히 수정하시오.\n","  ## X의 전체 평균이 아닌, 각 열마다 평균을 구해야한다.\n","  ## NumPy의 broadcasting을 이용하면 한 줄로 계산할 수 있다.\n","  #### 코드 시작 ####\n","  mean = np.zeros((n))\n","  #### 코드 종료 ####\n","\n","  ## X의 각 열에 대한 표준편차를 계산하여 변수 std에 저장할 수 있도록,\n","  ## 아래 코드를 적절히 수정하시오.\n","  ## X의 전체 평균이 아닌, 각 열마다 평균을 구해야한다.\n","  ## NumPy의 broadcasting을 이용하면 한 줄로 계산할 수 있다.\n","  #### 코드 시작 ####\n","  std = np.zeros((n))\n","  #### 코드 종료 ####\n","\n","  ## X의 값에 대해 mean normalization을 수행한다.\n","  ## NumPy의 broadcasting을 이용하면 한 줄로 계산할 수 있다.\n","  ## 아래 코드를 적절히 수정하시오.\n","  #### 코드 시작 ####\n","  X_scaling = np.zeros_like(X)\n","  #### 코드 종료 ####\n","\n","  return X_scaling, mean, std"]},{"cell_type":"markdown","metadata":{"id":"DO1XObkfAb5O"},"source":["Feature scaling을 한 후의 x<sub>1</sub>과 x<sub>2</sub>는 평균이 0, 표준편차가 1에 가까워야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzYfsf34_Z0L"},"outputs":[],"source":["# 평균 mean과 표준편차 std를 기억해 놓는다.\n","# 이 뒤부터는 X 대신 X_scaling을 이용한다.\n","X_scaling, mean, std = feature_scaling(X)\n","\n","# 아래는 테스트 코드이다.\n","assert abs(mean[0] - 2000.68085106) < 0.0001\n","assert abs(mean[1] - 3.17021277) < 0.0001\n","assert abs(std[0] - 7.86202619e+02) < 0.0001\n","assert abs(std[1] - 7.52842809e-01) < 0.0001\n","\n","print(\"성공!\")"]},{"cell_type":"markdown","metadata":{"id":"JqANRlQVHCpG"},"source":["## x<sub>0</sub> 열 추가\n","여러 개의 feature를 가지는 linear regression에서는 수식을 단순하게 만들기 위해 x<sub>0</sub>=1의 값을 가지는 열을 추가한다. 이는 model 함수를 행렬로 단순하게 표현할 수 있게 한다.\n","\n","$f(\\theta)=\\begin{bmatrix}\\theta_0 & \\theta_1 & ... & \\theta_n\\end{bmatrix}\n","\\begin{bmatrix}x_0=1 \\\\ x_1 \\\\ ... \\\\ x_n\\end{bmatrix}=\\theta^T x$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VpumZiOAI0kX"},"outputs":[],"source":["X_bias = np.ones((m, n+1))\n","X_bias[:, 1:] = X_scaling\n","\n","print(X_bias[:5,:])"]},{"cell_type":"markdown","metadata":{"id":"hmFlPZAtJndV"},"source":["## Model 함수\n","집값을 예측하기 위한 model 함수를 정의한다. Model 함수는 다음과 같은 형태를 갖는다.\n","\n","$f(\\theta)=\\begin{bmatrix}\\theta_0 & \\theta_1 & ... & \\theta_n\\end{bmatrix}\n","\\begin{bmatrix}x_0=1 \\\\ x_1 \\\\ ... \\\\ x_n\\end{bmatrix}=\\theta^T x$\n","\n","이는 $\\theta$와 x의 행렬곱 또는 내적을 이용해 쉽게 계산할 수 있다.\n","\n","행렬곱 또는 내적은 NumPy의 [dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) 함수를 이용하면 된다.\n","\n","**지시: Model 함수를 계산하는 아래 함수를 완성한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLsFOVxtKNMm"},"outputs":[],"source":["def compute_model(X_bias, theta):\n","  '''\n","  입력 변수 X_bias와 theta에 대해 model 함수의 값을 계산하여 반환하는 함수\n","\n","  매개변수:\n","    X_bias: x0가 추가된 입력 변수, NumPy 2차원 배열\n","    theta: parameter 값, NumPy 1차원 배열\n","\n","  반환:\n","    Model 함수의 값, NumPy 1차원 배열\n","  '''\n","\n","  ## X_bias와 theta를 이용하여 model 함수의 값을 계산한다.\n","  ## 이를 위해 아래 코드를 적절히 수정하시오.\n","  #### 코드 시작 ####\n","  f = np.zeros((m))\n","  #### 코드 종료 ####\n","\n","  return f"]},{"cell_type":"markdown","metadata":{"id":"VhfLE6kHWUEd"},"source":["아래 코드는 테스트 코드이다. \"성공!\"이라고 출력되어야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lo64WP-VKV6H"},"outputs":[],"source":["# 함수 compute_model()의 테스트 코드\n","\n","f_temp1 = compute_model(X_bias, [0, 0, 0])\n","assert f_temp1.size == 47\n","assert f_temp1.shape == (47,)\n","assert np.all(f_temp1 == 0)\n","\n","f_temp2 = compute_model(X_bias, [0.1, 2, 0.5])\n","assert f_temp2.size == 47\n","assert f_temp2.shape == (47,)\n","assert abs(f_temp2[0] - 0.24978416) < 0.0001\n","assert abs(f_temp2[1] + 1.03232808) < 0.0001\n","\n","print(\"성공!\")"]},{"cell_type":"markdown","metadata":{"id":"eWcuRdy-KhLY"},"source":["## Loss 함수\n","Model 함수의 parameter를 찾는데 사용할 loss 함수를 계산한다. Gradient descent 알고리즘에서는 loss 함수를 직접 이용하지는 않는다. 그러나 수렴성을 확인하기 위한 목적으로 loss 값을 저장한다.\n","\n","Loss 함수는 다음과 같이 계산할 수 있다.\n","\n","$L(\\theta)=\\cfrac{1}{2m}\\sum_{i=1}^{m}(f_{\\theta}(x^{(i)})-y^{(i)})^2$\n","\n","합계는 NumPy의 [sum](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) 함수를 이용하면 쉽게 계산할 수 있다.\n","\n","**지시: Loss 함수를 계산하는 아래 함수를 완성한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KU85CO25Tg7q"},"outputs":[],"source":["def compute_loss(X_bias, Y, theta):\n","  '''\n","  theta에 대해 loss 함수의 값을 계산하여 반환하는 함수\n","\n","  매개변수:\n","    X_bias: x0가 추가된 입력 변수, NumPy 2차원 배열\n","    Y: 출력 변수, NumPy 1차원 배열\n","    theta: parameter 값, NumPy 1차원 배열\n","\n","  반환:\n","    theta에서의 loss 값, 스칼라\n","  '''\n","\n","  m = Y.size # 데이터의 갯수\n","\n","  ## X_bias와 theta를 이용하여 model 함수의 값을 계산한다.\n","  ## 이 때, 앞에서 구현한 compute_model() 함수를 이용한다.\n","  ## 이를 위해 아래 코드를 적절히 수정하시오.\n","  #### 코드 시작 ####\n","  f = np.zeros((m))\n","  #### 코드 종료 ####\n","\n","  ## Y와 f를 이용하여 loss 값을 계산한다.\n","  ## 이를 위해 아래 코드를 적절히 수정하시오.\n","  #### 코드 시작 ####\n","  L = 0\n","  #### 코드 종료 ####\n","\n","  return L"]},{"cell_type":"markdown","metadata":{"id":"eTu8JxD-WNSt"},"source":["아래 코드는 테스트 코드이다. \"성공!\"이라고 출력되어야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAKN1jhLVEPS"},"outputs":[],"source":["# 함수 compute_loss()의 테스트 코드\n","\n","L_temp1 = compute_loss(X_bias, Y, [0, 0, 0])\n","assert abs(L_temp1 - 65591548106.45744) < 0.0001\n","\n","L_temp2 = compute_loss(X_bias, Y, [3, 2, 5])\n","assert abs(L_temp2 - 65590041820.70264) < 0.0001\n","\n","print(\"성공!\")"]},{"cell_type":"markdown","metadata":{"id":"r_j1CdvoVaQc"},"source":["## Gradient 함수\n","Gradient descent 방법을 이용하기 위해서는 loss 함수의 gradient를 계산해야 한다.\n","\n","Gradient는 다음과 같이 계산할 수 있다.\n","\n","$\\frac{\\partial}{\\partial \\theta_j}L(\\theta)=\\cfrac{1}{m}\\sum_{i=1}^{m}(f_{\\theta}(x^{(i)})-y^{(i)}) x_{j}^{(i)}$\n","\n","위의 gradient 계산 식은 NumPy의 [dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) 함수를 이용하면 쉽게 계산할 수 있다.\n","\n","**지시: Loss의 gradient를 계산하는 아래 함수를 완성한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7zdt1sBPXp4X"},"outputs":[],"source":["def compute_gradient(X_bias, Y, theta):\n","  '''\n","  theta에 대해 loss의 gradient 값을 계산하여 반환하는 함수\n","\n","  매개변수:\n","    X_bias: x0가 추가된 입력 변수, NumPy 2차원 배열\n","    Y: 출력 변수, NumPy 1차원 배열\n","    theta: parameter 값, NumPy 1차원 배열\n","\n","  반환:\n","    theta에서의 loss의 gradient 값, NumPy 1차원 배열\n","  '''\n","\n","  m, n = X.shape # 데이터 개수와 feature의 수\n","\n","  ## X_bias와 theta를 이용하여 model 함수의 값을 계산한다.\n","  ## 이 때, 앞에서 구현한 compute_model() 함수를 이용한다.\n","  ## compute_loss() 함수에서도 동일한 값을 계산한 적이 있다.\n","  ## 이를 위해 아래 코드를 적절히 수정하시오.\n","  #### 코드 시작 ####\n","  f = np.zeros((m))\n","  #### 코드 종료 ####\n","\n","  ## h와 X_bias, Y를 이용해 gradient를 계산한다.\n","  ## gradient는 np.dot을 이용하면 쉽게 계산할 수 있다.\n","  ## 이를 위해 아래 코드를 적절히 수정하시오.\n","  #### 코드 시작 ####\n","  dL = np.zeros((n+1))\n","  #### 코드 종료 ####\n","\n","  return dL"]},{"cell_type":"markdown","metadata":{"id":"QUXm9epFanRL"},"source":["아래 코드는 테스트 코드이다. \"성공!\"이라고 출력되어야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-P9Q6Ydya9a6"},"outputs":[],"source":["# 함수 compute_gradient()의 테스트 코드\n","\n","dL_temp1 = compute_gradient(X_bias, Y, [0, 0, 0])\n","assert abs(dL_temp1[0] + 340412.65957447) < 0.0001\n","assert abs(dL_temp1[1] + 105764.13349282) < 0.0001\n","assert abs(dL_temp1[2] + 54708.82175587) < 0.0001\n","\n","dL_temp2 = compute_gradient(X_bias, Y, [3, 2, 5])\n","assert abs(dL_temp2[0] + 340409.65957447) < 0.0001\n","assert abs(dL_temp2[1] + 105759.33365679) < 0.0001\n","assert abs(dL_temp2[2] + 54702.70182146) < 0.0001\n","\n","print(\"성공!\")"]},{"cell_type":"markdown","metadata":{"id":"wJOLfG6ibFVw"},"source":["## Gradient Descent\n","수업 시간에 배운 내용을 바탕으로 gradient descent 알고리즘을 구현한다. Gradient descent 알고리즘의 핵심은 다음 식을 일정 횟수만큼 반복하여 계산하는 것이다.\n","\n","$\\theta^{(k+1)}=\\theta^{(k)}-a\\nabla L(\\theta^{(k)})$\n","\n","**지시: Gradient descent 알고리즘을 수행하는 아래 코드를 완성한다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBWoEvBJc_2S"},"outputs":[],"source":["# Hyperparameter를 설정한다.\n","# 사용자가 변경할 수 있다.\n","iterations = 1000 # 반복 회수\n","learning_rate = 0.1 # Learning rate\n","theta = np.zeros((n+1,)) # theta의 초기값은 0으로 설정\n","\n","# 학습 상황을 저장하기 위한 변수\n","history = np.zeros((iterations + 1, n+1))\n","history[0, :n+1] = theta\n","history[0, -1] = compute_loss(X_bias, Y, theta)\n","\n","for iter in range(iterations):\n","  ## gradient descent의 반복식을 나타내기 위해,\n","  ## 아래 코드를 적절히 수정하시오.\n","  #### 코드 시작 ####\n","  theta = theta - learning_rate * compute_gradient(X_bias, Y, theta)\n","  #### 코드 종료 ####\n","\n","  # 수렴성을 확인하기 위해 매 iteration마다 loss를 계산한다.\n","  loss = compute_loss(X_bias, Y, theta)\n","\n","  # 그래프 출력을 위해 중간 결과를 저장한다.\n","  history[iter + 1, :n+1] = theta\n","  history[iter + 1, -1] = loss\n","\n","# 아래 결과가 [340412.65957447 109447.79646964  -6578.35485416]과 비슷하게 나와야 한다.\n","print(\"Theta found by gradient descent:\", theta)"]},{"cell_type":"markdown","metadata":{"id":"AWkdC_ZUketE"},"source":["아래 코드는 매 iteration 마다 loss가 변하는 과정을 그래프로 보여준다. Iteration이 진행됨에 따라서 loss가 감소해야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxlevC1jeVaf"},"outputs":[],"source":["def plot_loss_history(history):\n","  x = np.arange(0, history.shape[0])\n","  y = history[:, -1]\n","\n","  fig, axes = plt.subplots()\n","  axes.plot(x, y)\n","  axes.set_xlabel(\"Iteration\")\n","  axes.set_ylabel(\"Loss\")\n","\n","plot_loss_history(history)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"29cJ2rcrlg59"},"source":["현재 찾은 $\\theta$ 값을 이용해 집값을 예측해 볼 수 있다. 크기가 1,650(ft<sup>2</sup>)이고 방의 개수가 3개인 경우의 집값을 예측해보자. 단, x의 값으로 1,650과 3을 그대로 사용하면 안되고, 앞에서 구한 평균과 표준편차를 반영해 주어야 한다. 또한, x<sub>0</sub>=0인 값을 추가해 주어야 한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWT_zOQ6lvxK"},"outputs":[],"source":["# 집값을 예측하기 위한 코드\n","def predict(mean, std, theta, x):\n","  x_scaling = (x - mean) / std\n","  x_bias = np.ones((x.size+1))\n","  x_bias[1:] = x_scaling\n","\n","  f = compute_model(x_bias, theta)\n","  return f\n","\n","x_predict = np.array([1650.0, 3])\n","y_predict = predict(mean, std, theta, x_predict)\n","print(\"크기 1650, 방의 개수 3인 집의 가격은 $\", y_predict, \"입니다\")"]},{"cell_type":"markdown","metadata":{"id":"4yz-0710kcBC"},"source":["이번 과제는 여기까지 입니다. 수고하셨습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"isQd2aVsksEw"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1V_yq1Wb2zIyBimWjYIkhtm8gri7g5KeO","timestamp":1616243608063}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}